# Benchmarking on adaptive banded DP

This repository contains bencmarking (recall benchmark and speed benchmark) scripts used in the adaptive banded paper. If you are thinking of porting (or implementing) adaptive banded algorithm to your program, you should consider using [GABA library](https://github.com/ocxtal/libgaba) that implements the algorithm with another acceleration algorithm (it is much faster and much more stable).


## Contents

Several algorithms calculating semi-global alignment and benchmarking scripts are included.

* Naive, full-sized semi-global alignment with affine-gap penalty model.
* Adaptive banded DP with affine-gap penalty. (acceptable bandwidth is multiple of 8, determined at compile time with -DBW=32)
* Re-implementation of the semi-gapped alignment function in the NCBI BLAST+ package.
* SIMD parallelized variant of the BLAST semi-gapped alignment function.
* Myers' wavefront algorithm (with some heuristics, described in the DALIGNER paper), extracted from the [DALIGNER](https://github.com/thegenemyers/DALIGNER) repository.


## Build

`make` will generate all the binaries needed in the benchmarks with gcc compiler.


## Run benchmarks


### Recall benchmarks


#### Dependencies

The benchmark script are written in python with numpy library and internally calls [PBSIM reads simulator](http://bioinformatics.oxfordjournals.org/content/29/1/119.short). Optionally it uses Sun Grid Engine to run tasks on cluster computing environments. Figure generation script depends on python with matplotlib and R (rscript command) with ggplot2.


#### General evaluation

Running `scripts/evaluate.py` generates recall rate evaluation with various sequence lengths, sequence identities, band widths (of the adaptive banded algorithm) and mismatch / gap penalties. The results of the three figures, BW-identity plot and length-identity plot with BW=16 and BW=32, are generated with this script. The script invokes naive algorithm and dynamic banded algorithm in `../bin` directory (binaries must be generated beforehand) and puts the results into a tsv-formatted file.

The parametes, a set of ranges of sequence lengths, sequence identities, band widths, and mismatch / gap penalties, are stored in `scripts/params.py`. `generate_params.py` will calculate all the combination of the parameters and print them. The list of combination of parameters is needed as an input parameter of `evaluate.py`.

The results generated by `evaluate.py` is a large tsv-formatted file, can be aggregated (and shrinked) into python-readable text file with `aggregate.py`. The output will contain multi-dimensional (dimension the same as the input parameters) list in python format. You can convert the content into python list with `eval` function.

```
$ python scripts/generate_params.py > params.tsv
$ python scripts/evaluate.py params.tsv out.tsv
$ python scripts/aggregate.py out.tsv result.txt
```

In python, `result.txt` can be converted to `numpy.array` object. `l` and `a` holds the results of linear-gap and affine-gap algorithms, respectively.

 
```
>>> from scripts.util import *
>>> (l, a) = load_result('result.txt')
```

The whole evaluation task is very heavy (mainly due to the slow matrix calculation in the naive, full-sized implementation), you can run scripts in a distributed mannar with Sun Grid Engines. The `sim.sh` script carry outs parameter generation and evaluation task distribution with `qsub` command. Multiple output files will be generated, prefixed with task numbers, can be aggregated into `result.txt` with `aggr.sh`.


```
$ cd path/to/work
$ ../path/to/scripts/sim.sh
$ ../path/to/scripts/aggr.sh
```

The `results/` directory contains aggregated results of the benchmark with gap extension penalty Ge = 0 (linear), 1, 2, and 3.


#### Gap penalty evaluation

The figure 3(b), recall rate with different gap panalties, was generated by other scripts, `generate_params_gige.py`, `evaluate_gige.py` and `aggregate_id_gige.py`. The script runs similar evaluation tasks to the general evaluation, different in that the mismatch penalty is fixed to 2 and both gap open and gap extension penalties are varied. The results are placed in `results/`, named `result_gap_linear.txt` and `result_gap_affine.txt`.


#### Indel tolerance evaluation

The figure 3(d), recall rate with different gap insertion size, was generated with `generate_params_gap.py`, `evaluate_gap.py` and `aggregate_gap.py`. The scripts generates simulated reference-read pairs with a gap inserted on the reference sequence and recall rate evaluation with parameters similar to the general evaluation. The result is placed in `results/` with the name `gige_id_ddiag_table.txt`.



### Speed benchmark

`scripts/bench.sh` runs `../bin/bench` with sequence lengths varied within [100, 10k].


### Figure generation


The results used in the paper is contained in results/ directory and plotted to eps files with  `plot.py` and `plot.r`. The `plot.py` scripts open the result files in `../results` directory and perform some preprocesses required in `plot.r`.

```
$ cd results
$ python ../scripts/plot.py
$ rscript ../scripts/plot.r
```


## License

Apache v2.

Copyright (c) 2015-2016 Hajime Suzuki






